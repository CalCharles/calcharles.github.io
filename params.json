{
  "name": "Comparing Human-Centric and Robot-Centric Sample Efficiency for Robot Deep Learning from Demonstrations",
  "tagline": "",
  "body": "### Abstract\r\nMotivated by recent advances in Deep Learning for robot control, this paper considers two learning algorithms in terms of how they acquire demonstrations. “Human-Centric” (HC) sampling is the standard supervised learning algorithm, where a human supervisor demonstrates the task by teleop- erating the robot to provide trajectories consisting of state- control pairs. “Robot-Centric” (RC) sampling is an increasingly popular alternative used in algorithms such as DAgger, where a human supervisor observes the robot executing a learned policy and provides corrective control labels for each state visited. RC sampling can be tedious for human supervisors and prone to mislabeling. RC sampling can also induce error in policy performance because it repeatedly visits areas of the state space that are avoided once the task is learned. Although policies learned with RC sampling can be superior to HC sampling for standard learning models such as linear SVMs, policies learned with HC sampling perform equally well or better with emerging classes of highly-expressive learning models such as deep learning and hyper-parametric decision trees. We compare HC and RC using a simulated grid world and a physical robot singulation task where the input is a binary image of a connected set of objects on a planar worksurface and the policy generates a motion of the gripper to separate one object from the rest. In HC, humans demonstrate the task by tele-operating a four-axis robot and in RC, humans provide corrective labels to trajectories generated by the robot after an initial training phase. Performance for a test instance is considered successful when a part is separated on all sides by a 10.0cm zone after the motion is completed. We find that for linear SVMs, policies learned with RC to those learned with HC but that with deep models this advantage disappears and that deep models trained with HC can perform significantly better than with RC. We also find that with RC, the corrective control labels provided by humans are highly inconsistent (uncorrelated with their own previous labels). We then prove that there exists a class of examples where in the limit, HC is guaranteed to converge to an optimal policy while RC may fail to converge. These results suggest the surprising result that HC sampling may be preferable for highly-expressive learning models like deep learning.\r\n\r\n### Authors and Contributors\r\nMichael Laskey, Caleb Chuck, Jonathan Lee, Jeffrey Mahler, Sanjay Krishnan, Kevin Jamieson, Anca Dragan, Ken Goldberg\r\n\r\n### Support or Contact\r\nPlease contact Michael Laskey (mdlaskey AT berkeley DOT edu) for further information.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}